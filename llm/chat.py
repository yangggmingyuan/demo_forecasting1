"""èŠå¤©åŠŸèƒ½æ¨¡å—"""
import streamlit as st
from typing import List, Dict
from llm.gemini import call_gemini
from llm.qwen_local import call_qwen_local


def get_page_context() -> str:
    """è·å–å½“å‰é¡µé¢ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äº LLM æç¤º"""
    context = f"å½“å‰é¡µé¢: {st.session_state.get('current_page', 'Home')}\n"
    
    df = st.session_state.get('df_data')
    if df is not None:
        context += f"æ•°æ®æ¦‚è§ˆ: å…± {len(df)} è¡Œè®°å½•\n"
        context += f"æ•°æ®åˆ—: {', '.join(df.columns.tolist())}\n"
    return context


def chat_with_llm(user_message: str, provider: str = 'qwen_local') -> str:
    """ä¸ LLM è¿›è¡Œå¯¹è¯"""
    # è·å–é¡µé¢ä¸Šä¸‹æ–‡
    page_context = get_page_context()
    
    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¾›åº”é“¾æ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£ä¾›åº”é“¾æ•°æ®å’Œåˆ†æç»“æœã€‚

å½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯:
{page_context}

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€æ¸…æ™°çš„åˆ†æå’Œå»ºè®®ã€‚"""
    
    # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨ï¼ˆå¦‚æœä¸ºç©ºï¼‰
    if 'chat_messages' not in st.session_state:
        st.session_state['chat_messages'] = []

    # ç¡®ä¿ç¬¬ä¸€æ¡æ˜¯ System Prompt
    if not st.session_state['chat_messages'] or st.session_state['chat_messages'][0].get('role') != 'system':
         st.session_state['chat_messages'].insert(0, {'role': 'system', 'content': system_prompt})
    else:
         st.session_state['chat_messages'][0]['content'] = system_prompt

    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state['chat_messages'].append({
        'role': 'user',
        'content': user_message
    })
    
    # è°ƒç”¨ LLM
    response = None
    
    # å¼ºåˆ¶ä½¿ç”¨ Qwen Local
    # Local Qwen Logic
    api_base = st.session_state.get('qwen_api_base', 'http://localhost:11434/v1')
    model_name = st.session_state.get('qwen_model_name', 'qwen2.5:14b')
    
    # Auto-detect model if we are using a likely incorrect default or if previous call failed
    # We do this check once if the model name seems like a default guess
    from llm.qwen_local import get_ollama_models
    
    # Check if we should try to auto-detect (e.g., first run or default value)
    if 'qwen_model_autodetected' not in st.session_state:
        available_models = get_ollama_models(api_base)
        if available_models:
            # Try to find a qwen model
            best_match = next((m for m in available_models if 'qwen' in m.lower()), None)
            if best_match:
                model_name = best_match
            else:
                model_name = available_models[0] # Fallback to whatever is installed
            
            # Update session state so we use this valid model
            st.session_state['qwen_model_name'] = model_name
            st.session_state['qwen_model_autodetected'] = True
    
    response = call_qwen_local(
        messages=st.session_state['chat_messages'],
        api_base=api_base,
        model_name=model_name
    )
    
    # æ·»åŠ åŠ©æ‰‹å›å¤
    if response:
        st.session_state['chat_messages'].append({
            'role': 'assistant',
            'content': response
        })
    
    return response


def render_floating_chat():
    """
    æ¸²æŸ“æµ®åŠ¨èŠå¤©çª—å£
    ä¿®å¤ç‰ˆï¼šä½¿ç”¨ st.popover + st.text_input + st.buttonï¼Œè§£å†³ st.chat_input åœ¨ popover å†…ä¸å·¥ä½œçš„é—®é¢˜ã€‚
    """
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šåŠ å…¥è¿™æ®µ CSS æ¥æ”¾å¤§æŒ‰é’® ---
    st.markdown("""
    <style>
        /* æ‰¾åˆ°æ‰€æœ‰çš„ Popover æŒ‰é’®å¹¶æ”¾å¤§ */
        div[data-testid="stPopover"] > button {
            font-size: 3rem !important;  /* å›¾æ ‡/å­—ä½“å¤§å°ï¼šè¿™é‡Œæ”¹æˆäº† 3å€å¤§å° */
            width: 80px !important;      /* æŒ‰é’®å®½åº¦ */
            height: 80px !important;     /* æŒ‰é’®é«˜åº¦ */
            border-radius: 50% !important; /* å¯é€‰ï¼šè®¾ä¸º 50% ä¼šå˜æˆåœ†å½¢æŒ‰é’®ï¼Œä¸å†™è¿™è¡Œå°±æ˜¯åœ†è§’çŸ©å½¢ */
            border: 2px solid #4e73df !important; /* å¯é€‰ï¼šåŠ ä¸ªè¾¹æ¡†é¢œè‰² */
            background-color: white !important; /* Ensure background is visible if gradient fails */
            box-shadow: 0 4px 10px rgba(0,0,0,0.2) !important;
        }
        
        /* Position Fix (Optional, ensuring it stays in bottom right) */
        div[data-testid="stPopover"] {
            position: fixed !important;
            bottom: 40px !important;
            right: 40px !important;
            z-index: 99999 !important;
        }
    </style>
    """, unsafe_allow_html=True)
    # -------------------------------------
    
    # å¼ºåˆ¶ Provider ä¸º qwen_local
    st.session_state['llm_provider'] = 'qwen_local'

    with st.popover("ğŸ¤–", help="AI æ™ºèƒ½åŠ©æ‰‹"):
        st.markdown(
            """
            <div style="
                padding: 15px 0px; 
                font-weight: 800; 
                font-size: 1.4rem; 
                border-bottom: 2px solid #f0f0f0; 
                margin-bottom: 15px;
                background: linear-gradient(to right, #4e73df, #36b9cc);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                display: flex;
                align-items: center;
                gap: 10px;
            ">
                âœ¨ ä¾›åº”é“¾æ•°æ®åŠ©æ‰‹ <span style="-webkit-text-fill-color: #888; font-size: 0.8rem; font-weight: normal; margin-left: auto;">(Local AI)</span>
            </div>
            """, 
            unsafe_allow_html=True
        )
        


        # Chat Container
        chat_container = st.container(height=350)
        
        with chat_container:
            messages_to_show = [msg for msg in st.session_state.get('chat_messages', []) if msg.get('role') != 'system']
            if not messages_to_show:
                st.info("ğŸ‘‹ æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æœ¬åœ°æ•°æ®åˆ†æåŠ©æ‰‹ã€‚")
            
            for msg in messages_to_show:
                with st.chat_message(msg['role'], avatar="ğŸ§‘â€ğŸ’»" if msg['role'] == "user" else "ğŸ¤–"):
                    st.markdown(msg['content'])

        # Input Area
        with st.form(key="chat_form", clear_on_submit=True):
            col1, col2 = st.columns([4, 1])
            with col1:
                user_input = st.text_input(
                    "è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    key="chat_input_text",
                    label_visibility="collapsed",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."
                )
            with col2:
                submitted = st.form_submit_button("å‘é€", use_container_width=True)
            
            if submitted and user_input and user_input.strip():
                prompt = user_input.strip()
                
                # Show spinner while thinking
                with st.spinner("æ€è€ƒä¸­..."):
                    chat_with_llm(prompt, provider='qwen_local')
                st.rerun()

